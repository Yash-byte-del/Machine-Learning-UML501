{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cb6248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2226768f",
   "metadata": {},
   "source": [
    "## Q1 SMS Spam Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80084228",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=['https://media.geeksforgeeks.org/wp-content/uploads/20240515170038/spam.csv',\n",
    "      'https://raw.githubusercontent.com/epfml/ML_course/master/labs/ex11_adaboost/spam.csv',\n",
    "      'https://raw.githubusercontent.com/justmarkham/pydata-book/master/datasets/sms_spam/spam.csv']\n",
    "for u in urls:\n",
    "    try:\n",
    "        df=pd.read_csv(u, encoding='latin-1')\n",
    "        break\n",
    "    except:\n",
    "        df=None\n",
    "if df is None:\n",
    "    print('CSV not found; please place spam.csv in working dir')\n",
    "else:\n",
    "    if 'v1' in df.columns:\n",
    "        df=df.rename(columns={'v1':'label','v2':'text'})\n",
    "    df=df[['label','text']]\n",
    "    df['y']=df['label'].map({'ham':0,'spam':1})\n",
    "    X_text=df['text']\n",
    "    tf=TfidfVectorizer(lowercase=True,stop_words='english')\n",
    "    X=tf.fit_transform(X_text)\n",
    "    X_train,X_test,y_train,y_test=train_test_split(X,df['y'],test_size=0.2,random_state=42,stratify=df['y'])\n",
    "    print('Loaded SMS dataset, shape',df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3db991",
   "metadata": {},
   "outputs": [],
   "source": [
    "stump=DecisionTreeClassifier(max_depth=1, random_state=0)\n",
    "stump.fit(X_train,y_train)\n",
    "print('train',accuracy_score(y_train,stump.predict(X_train)))\n",
    "print('test',accuracy_score(y_test,stump.predict(X_test)))\n",
    "print(confusion_matrix(y_test,stump.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043b6700",
   "metadata": {},
   "source": [
    "### Manual AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d89bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_adaboost(X_train,y_train,X_test,T=15):\n",
    "    n=X_train.shape[0]\n",
    "    w=np.ones(n)/n\n",
    "    learners=[]\n",
    "    alphas=[]\n",
    "    errs=[]\n",
    "    for t in range(T):\n",
    "        clf=DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "        clf.fit(X_train,y_train,sample_weight=w)\n",
    "        pred=clf.predict(X_train)\n",
    "        miss=(pred!=y_train).astype(int)\n",
    "        err=np.sum(w*miss)/np.sum(w)\n",
    "        if err==0:\n",
    "            alpha=1\n",
    "        else:\n",
    "            alpha=0.5*np.log((1-err)/err)\n",
    "        w=w*np.exp(alpha*(miss*2-1))\n",
    "        w=w/w.sum()\n",
    "        learners.append(clf)\n",
    "        alphas.append(alpha)\n",
    "        errs.append(err)\n",
    "        mis_idx=np.where(miss==1)[0]\n",
    "        print('iter',t+1,'err',round(err,4),'alpha',round(alpha,4),'mis_count',len(mis_idx))\n",
    "    def predict_ensemble(X):\n",
    "        agg=np.zeros(X.shape[0])\n",
    "        for a,clf in zip(alphas,learners):\n",
    "            agg+=a*(clf.predict(X)*2-1)\n",
    "        return (agg>0).astype(int)\n",
    "    ytr_pred=predict_ensemble(X_train)\n",
    "    yte_pred=predict_ensemble(X_test)\n",
    "    print('train',accuracy_score(y_train,ytr_pred))\n",
    "    print('test',accuracy_score(y_test,yte_pred))\n",
    "    print(confusion_matrix(y_test,yte_pred))\n",
    "    plt.figure();plt.plot(range(1,T+1),errs);plt.xlabel('iter');plt.ylabel('weighted error');plt.show()\n",
    "    plt.figure();plt.plot(range(1,T+1),alphas);plt.xlabel('iter');plt.ylabel('alpha');plt.show()\n",
    "manual_adaboost(X_train,y_train,X_test,T=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1ee2a6",
   "metadata": {},
   "source": [
    "### Sklearn AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78df9b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab=AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),n_estimators=100,learning_rate=0.6,random_state=0)\n",
    "ab.fit(X_train,y_train)\n",
    "print('train',accuracy_score(y_train,ab.predict(X_train)))\n",
    "print('test',accuracy_score(y_test,ab.predict(X_test)))\n",
    "print(confusion_matrix(y_test,ab.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e35a56",
   "metadata": {},
   "source": [
    "## Q2 Heart Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb534f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_urls=['https://raw.githubusercontent.com/dev-kudli/heart-disease-prediction/dev/heart.csv',\n",
    "            'https://raw.githubusercontent.com/anshuldhingra/heart-disease-prediction/master/heart.csv',\n",
    "            'https://raw.githubusercontent.com/auribises/Heart-Disease-Dataset/master/heart.csv']\n",
    "for u in heart_urls:\n",
    "    try:\n",
    "        heart=pd.read_csv(u)\n",
    "        break\n",
    "    except:\n",
    "        heart=None\n",
    "if heart is None:\n",
    "    print('heart CSV not found; please place heart.csv in working dir')\n",
    "else:\n",
    "    if 'target' in heart.columns:\n",
    "        y=heart['target']\n",
    "        X=heart.drop(columns=['target'])\n",
    "    elif 'HeartDisease' in heart.columns:\n",
    "        y=heart['HeartDisease']\n",
    "        X=heart.drop(columns=['HeartDisease'])\n",
    "    else:\n",
    "        y=heart.iloc[:,-1]\n",
    "        X=heart.iloc[:,:-1]\n",
    "    X_train_h,X_test_h,y_train_h,y_test_h=train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)\n",
    "    stump_h=DecisionTreeClassifier(max_depth=1,random_state=0)\n",
    "    stump_h.fit(X_train_h,y_train_h)\n",
    "    print('stump train',accuracy_score(y_train_h,stump_h.predict(X_train_h)))\n",
    "    print('stump test',accuracy_score(y_test_h,stump_h.predict(X_test_h)))\n",
    "    print(confusion_matrix(y_test_h,stump_h.predict(X_test_h)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd76a88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_list=[5,10,25,50,100]\n",
    "lr_list=[0.1,0.5,1.0]\n",
    "res={}\n",
    "for lr in lr_list:\n",
    "    accs=[]\n",
    "    for n in n_list:\n",
    "        m=AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),n_estimators=n,learning_rate=lr,random_state=0)\n",
    "        m.fit(X_train_h,y_train_h)\n",
    "        accs.append(accuracy_score(y_test_h,m.predict(X_test_h)))\n",
    "    res[lr]=accs\n",
    "for lr,accs in res.items():\n",
    "    plt.plot(n_list,accs,label=f'lr={lr}')\n",
    "plt.xlabel('n_estimators');plt.ylabel('accuracy');plt.legend();plt.show()\n",
    "best_lr,max_acc=None,0\n",
    "for lr,accs in res.items():\n",
    "    m=max(accs)\n",
    "    if m>max_acc:\n",
    "        max_acc=m;best_lr=lr;best_n=n_list[accs.index(m)]\n",
    "print('best',best_lr,best_n,max_acc)\n",
    "best=AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),n_estimators=best_n,learning_rate=best_lr,random_state=0)\n",
    "best.fit(X_train_h,y_train_h)\n",
    "print('test',accuracy_score(y_test_h,best.predict(X_test_h)))\n",
    "errs=[]\n",
    "for est in best.estimators_:\n",
    "    pred=est.predict(X_train_h)\n",
    "    errs.append((pred!=y_train_h).mean())\n",
    "plt.plot(range(1,len(errs)+1),errs);plt.xlabel('iter');plt.ylabel('weak error');plt.show()\n",
    "importances=best.feature_importances_\n",
    "inds=np.argsort(importances)[::-1][:5]\n",
    "print('top5 features', list(X.columns[inds]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d352b138",
   "metadata": {},
   "source": [
    "## Q3 WISDM accelerometer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66fc85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wisdm_urls=['https://raw.githubusercontent.com/soham97/WISD_HAR_files/master/WISDM_ar_v1.1_raw.txt',\n",
    "           'https://raw.githubusercontent.com/laxmimerit/WISDM_datasets/master/WISDM_ar_v1.1_raw.txt']\n",
    "for u in wisdm_urls:\n",
    "    try:\n",
    "        txt=pd.read_csv(u,header=None,encoding='latin-1')\n",
    "        break\n",
    "    except:\n",
    "        txt=None\n",
    "if txt is None:\n",
    "    print('WISDM not found; place WISDM_ar_v1.1_raw.txt in working dir')\n",
    "else:\n",
    "    txt=txt[0].str.replace(';','',regex=False)\n",
    "    parts=txt.str.split(',',expand=True)\n",
    "    parts.columns=['user','activity','timestamp','x','y','z']\n",
    "    parts=parts.dropna()\n",
    "    parts['x']=parts['x'].astype(float)\n",
    "    parts['y']=parts['y'].astype(float)\n",
    "    parts['z']=parts['z'].astype(float)\n",
    "    mapping={'Jogging':1,'Upstairs':1,'Walking':0,'Sitting':0,'Standing':0,'Downstairs':0,'Jog':1,'Up':1}\n",
    "    parts['label']=parts['activity'].map(mapping).fillna(0).astype(int)\n",
    "    features=parts[['x','y','z']]\n",
    "    X_train_w,X_test_w,y_train_w,y_test_w=train_test_split(features,parts['label'],test_size=0.3,random_state=42,stratify=parts['label'])\n",
    "    stump_w=DecisionTreeClassifier(max_depth=1,random_state=0)\n",
    "    stump_w.fit(X_train_w,y_train_w)\n",
    "    print('stump train',accuracy_score(y_train_w,stump_w.predict(X_train_w)))\n",
    "    print('stump test',accuracy_score(y_test_w,stump_w.predict(X_test_w)))\n",
    "    def manual_adaboost_tab(X_train,y_train,X_test,T=20):\n",
    "        n=X_train.shape[0]\n",
    "        w=np.ones(n)/n\n",
    "        learners=[]\n",
    "        alphas=[]\n",
    "        errs=[]\n",
    "        Xn=X_train.values if hasattr(X_train,'values') else X_train\n",
    "        for t in range(T):\n",
    "            clf=DecisionTreeClassifier(max_depth=1)\n",
    "            clf.fit(Xn,y_train,sample_weight=w)\n",
    "            pred=clf.predict(Xn)\n",
    "            miss=(pred!=y_train).astype(int)\n",
    "            err=np.sum(w*miss)/np.sum(w)\n",
    "            alpha=0.5*np.log((1-err)/err) if err>0 else 1\n",
    "            w=w*np.exp(alpha*(miss*2-1))\n",
    "            w=w/w.sum()\n",
    "            learners.append(clf);alphas.append(alpha);errs.append(err)\n",
    "        def pred_ens(X):\n",
    "            agg=np.zeros(X.shape[0])\n",
    "            for a,clf in zip(alphas,learners):\n",
    "                agg+=a*(clf.predict(X)*2-1)\n",
    "            return (agg>0).astype(int)\n",
    "        print('train',accuracy_score(y_train,pred_ens(Xn)))\n",
    "        print('test',accuracy_score(y_test_w,pred_ens(X_test_w.values)))\n",
    "    manual_adaboost_tab(X_train_w.reset_index(drop=True),y_train_w.reset_index(drop=True),X_test_w,T=20)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
